{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  tratar nulos y outliers\n",
    "# escalar los valores (en este caso creo que basta con min max scaler)\n",
    "#  codificar la estacionalidad. A menudo se utilizan variables de fecha cíclicas (por ejemplo, seno/coseno del día del año) para capturar la estacionalidad anual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model, Sequential\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"data/stickers/train_preprocessed.csv\")\n",
    "# df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura combinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv(\"data/stickers/train_preprocessed.csv\")\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Identificar combinaciones únicas de país, producto y tienda\n",
    "combinaciones = df[['country', 'product', 'store']].drop_duplicates()\n",
    "\n",
    "# Parámetros\n",
    "window_size = 60  # Tamaño de la ventana de ventas anteriores\n",
    "\n",
    "# Almacenar secuencias para todas las series temporales\n",
    "X_sales, X_current, y = [], [], []\n",
    "\n",
    "df.dropna(inplace=True) # de momento los tiramos, ya veremos si se peude imputar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4320/4320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 21ms/step - loss: 0.0103 - val_loss: 0.0040\n",
      "Epoch 2/5\n",
      "\u001b[1m4320/4320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 22ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 3/5\n",
      "\u001b[1m4320/4320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 4/5\n",
      "\u001b[1m4320/4320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 5/5\n",
      "\u001b[1m4320/4320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0033\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Función para crear secuencias de ventas anteriores\n",
    "def create_sales_sequences(sales, window_size):\n",
    "    X = []\n",
    "    for i in range(len(sales) - window_size):\n",
    "        X.append(sales[i:i + window_size])\n",
    "    return np.array(X)\n",
    "\n",
    "# Procesar cada combinación única (serie temporal individual)\n",
    "for _, combo in combinaciones.iterrows():\n",
    "    subset = df[(df['country'] == combo['country']) & \n",
    "                (df['product'] == combo['product']) & \n",
    "                (df['store'] == combo['store'])]\n",
    "\n",
    "    # Ordenar por fecha\n",
    "    subset = subset.sort_values('date')\n",
    "\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "\n",
    "    # One hot encoding de variables categóricas\n",
    "    subset = pd.get_dummies(subset, columns=['country', 'product', 'store'])\n",
    "\n",
    "    # Extraer ventas y características del día actual\n",
    "    sales = subset['num_sold'].values\n",
    "    current_features = subset.drop(columns=['date', 'num_sold']).values\n",
    "\n",
    "    # Normalizar ventas y características\n",
    "    scaler_sales = MinMaxScaler()\n",
    "    scaler_features = MinMaxScaler()\n",
    "    \n",
    "    scaled_sales = scaler_sales.fit_transform(sales.reshape(-1, 1))\n",
    "    scaled_features = scaler_features.fit_transform(current_features)\n",
    "\n",
    "    # Crear secuencias de ventas anteriores\n",
    "    X_seq = create_sales_sequences(scaled_sales, window_size)\n",
    "    X_cur = scaled_features[window_size:]  # Características del día actual correspondientes\n",
    "    y_seq = scaled_sales[window_size:]  # Valores de ventas esperados\n",
    "\n",
    "    # print(\"X SEQ:\",X_seq)\n",
    "    # print(\"CURRENT:\",X_cur)\n",
    "    # print(\"Y SEQ:\",y_seq)\n",
    "\n",
    "    # Asegurar que las dimensiones coincidan\n",
    "    if len(X_seq) == len(X_cur) == len(y_seq):\n",
    "        X_sales.extend(X_seq)\n",
    "        X_current.extend(X_cur)\n",
    "        y.extend(y_seq)\n",
    "\n",
    "# Convertir listas a arrays numpy\n",
    "X_sales = np.array(X_sales)\n",
    "X_current = np.array(X_current)\n",
    "y = np.array(y)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento, validación y prueba\n",
    "train_size = int(len(X_sales) * 0.8)\n",
    "X_sales_train, X_sales_test = X_sales[:train_size], X_sales[train_size:]\n",
    "X_current_train, X_current_test = X_current[:train_size], X_current[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# VALIDACIÓN\n",
    "# train_size = int(len(X_sales_train) * 0.8)\n",
    "# X_sales_train, X_sales_val = X_sales[:train_size], X_sales_train[train_size:]\n",
    "# X_current_train, X_current_val = X_current_train[:train_size], X_current_train[train_size:]\n",
    "# y_train, y_val = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "\n",
    "# Construir el submodelo LSTM para las ventas anteriores\n",
    "input_sales = Input(shape=(window_size, 1))\n",
    "lstm_out = LSTM(50, activation='relu')(input_sales)\n",
    "\n",
    "# Construir el submodelo denso para las características actuales\n",
    "input_current = Input(shape=(X_current_train.shape[1],))\n",
    "dense_out = Dense(32, activation='relu')(input_current)\n",
    "\n",
    "# Combinar las salidas de ambos submodelos\n",
    "combined = Concatenate()([lstm_out, dense_out])\n",
    "output = Dense(1)(combined)\n",
    "\n",
    "# Definir el modelo completo\n",
    "model = Model(inputs=[input_sales, input_current], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit([X_sales_train, X_current_train], y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = model.predict([X_sales_test, X_current_test])\n",
    "\n",
    "# Invertir la normalización para obtener los valores originales\n",
    "predictions = scaler_sales.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406029.30675133306"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar predicciones\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mean_squared_error(predictions,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
